{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Text User Reviews\n",
    "### Predicting the user ratings based on the unstructured text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, LSTM, Activation, GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset\n",
    "* The input dataset contains two fields\n",
    "  * Reviews: The user entered mobile device reviews\n",
    "  * Rating: The user assigned rating to the mobile device 1 to 5 (Transformed to 0 to 4 by subtracting 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                            Reviews\n",
       "0       5  I feel so LUCKY to have found this used (phone...\n",
       "1       4  nice phone, nice up grade from my pantach revu...\n",
       "2       5                                       Very pleased\n",
       "3       4  It works good but it goes slow sometimes but i...\n",
       "4       4  Great phone to replace my lost phone. The only..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Mobile Review Rating.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Original Input Dataframe: (413840, 2)\n",
      "Shape of the truncated Input Dataframe: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Original Input Dataframe: \" + str(df.shape))\n",
    "\n",
    "# To speed up processing we will only use first 5000 records\n",
    "df = df.head(5000)\n",
    "\n",
    "print(\"Shape of the truncated Input Dataframe: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 texts. \n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = [] # list of label ids\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    texts.append(str(df['Reviews'][i]))\n",
    "    labels.append(df['Rating'][i] - 1)\n",
    "print ('Found %s texts. ' % len(texts))\n",
    "\n",
    "#Or texts = list(map(str, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Word Vectors\n",
    "* Using the GloVe 100 encoding\n",
    "\n",
    "### Additional Information Embedding Layer \n",
    "* The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "  * Input_dim: This is the size of the vocabulary in the text data. In our case this number is 10K so the size of the vocabulary would be 10K + 1 words.\n",
    "  * output_dim: This is the size of the vector space in which words will be embedded. In our case this is 100 as we are using GloVe 100.\n",
    "  * input_length: This is the length of input sequences, in our case this value is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"glove.6B.100d.txt\", 'r', encoding=\"utf8\")\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Preprocessing\n",
    "* Tokenization\n",
    "* Text cleaning\n",
    "* Integer encoding of the text data and word index creation to be later utilized in the embedding \n",
    "* The maximum sequence length (maxlen) is taken as 300\n",
    "* The vocab size is fixed to a vocab of 10000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9166 unique tokens.\n",
      "Shape of data tensor:  (5000, 300)\n",
      "Shape of labels tensor:  (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Functions provided by the Tokenizer class\n",
    "# [tokenizer.word_counts]: Give back the word count for each unique word\n",
    "# [tokenizer.document_count]: Give back the number of documents (in our case number of elements in the list)\n",
    "# [tokenizer.word_index]: A dictionary of words and their uniquely assigned integers\n",
    "# [tokenizer.word_docs]: A dictionary of words and how many documents each appeared in\n",
    "# num_words will not truncate the words found in the input but it will truncate the usage. \n",
    "# num_words is respected in the texts_to_sequences method which turns input into numerical arrays. (During dictionary creation\n",
    "# the tokenizer creates the complete dictionary. So in a way this is your vocab limit while converting text to sequences and \n",
    "# encoding sentences.)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000) # Vocab size 10000\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=300) # Maximum sequence length\n",
    "\n",
    "print ('Shape of data tensor: ', data.shape)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "#np.utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes-1) to one-hot vector.\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print ('Shape of labels tensor: ', labels.shape) #Here in our case ar the ratings are from 1-5 so we are getting one hot encoding 0-5 i.e. 6 classes\n",
    "\n",
    "#Another method for one hot encoding\n",
    "#labels = np.eye(5)[labels.reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and validation set\n",
    "* We will use 95% data for training the model\n",
    "* 5% data will be used as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting data into training and validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "df['Reviews'] = df['Reviews'][indices]\n",
    "nb_validation_samples = int(0.05 * data.shape[0]) #10% Test dataset\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (4750, 300)\n",
      "Shape of test dataset: (250, 300)\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape of training dataset: \" + str(x_train.shape))\n",
    "print (\"Shape of test dataset: \" + str(x_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding Layer\n",
    "* We will use embedding_index dictionary and word_index to compute embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Embedding Matrix :(9167, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100)) # 100 is the embedding dimension as we are using Globe 100\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(\"Shape of Embedding Matrix :\"  + str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Embedding Matrix into an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, #Adding 1 to fit Keras embedding requirements to accomodate unknown word\n",
    "                           100, #  100 is the embedding dimension as we are using Globe 100\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=300, # 300 is the maximum sequence length used in the padding function\n",
    "                           trainable=False) # Trainable false to prevent the weights from being updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"Weights[0][1][1]: \", + str(embedding_layer.get_weights()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functions of Embedding Layer\n",
    "  * Embedding layer is fed sequences of integers i.e. a 2D input of shape (samples, indices)\n",
    "  * These input sequences ideally should have the same lenght\n",
    "  * Embedding layes maps the integer inputs to the vectors found at the corresponding index in the embedding matrix\n",
    "  * Ex: sequence [1,2] will convert to [embeddings[1], embeddings[2]]\n",
    "  * The output of the embedding layer will be a 3D tensor of shape (batch size, max input length, dimensions of word vector)\n",
    "  * In our case (batch size, 300, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 100)          916700    \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 296, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 55, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 1,063,551\n",
      "Trainable params: 146,851\n",
      "Non-trainable params: 916,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(300,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "X = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "X = MaxPooling1D(5)(X)\n",
    "X = Conv1D(128, 5, activation='relu')(X)\n",
    "X = GlobalMaxPooling1D()(X)\n",
    "X = Dense(5, activation='relu')(X)\n",
    "X = Dense(5, activation='softmax')(X)\n",
    "model = Model(inputs=sequence_input, outputs=X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4750 samples, validate on 250 samples\n",
      "Epoch 1/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.9999 - acc: 0.6709 - val_loss: 1.0338 - val_acc: 0.6720\n",
      "Epoch 2/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.8782 - acc: 0.6859 - val_loss: 0.9412 - val_acc: 0.6880\n",
      "Epoch 3/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.7665 - acc: 0.7063 - val_loss: 0.9736 - val_acc: 0.6800\n",
      "Epoch 4/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.6935 - acc: 0.7183 - val_loss: 0.9463 - val_acc: 0.6960\n",
      "Epoch 5/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.6414 - acc: 0.7427 - val_loss: 0.9357 - val_acc: 0.6760\n",
      "Epoch 6/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.5593 - acc: 0.7935 - val_loss: 0.8709 - val_acc: 0.7440\n",
      "Epoch 7/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.4787 - acc: 0.8343 - val_loss: 0.8790 - val_acc: 0.7440\n",
      "Epoch 8/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.4154 - acc: 0.8604 - val_loss: 0.9465 - val_acc: 0.7360\n",
      "Epoch 9/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.3601 - acc: 0.8857 - val_loss: 0.9330 - val_acc: 0.7320\n",
      "Epoch 10/25\n",
      "4750/4750 [==============================] - 32s 7ms/step - loss: 0.3023 - acc: 0.9141 - val_loss: 0.9999 - val_acc: 0.7240\n",
      "Epoch 11/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.2755 - acc: 0.9196 - val_loss: 1.0453 - val_acc: 0.7480\n",
      "Epoch 12/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.2510 - acc: 0.9303 - val_loss: 0.9763 - val_acc: 0.7560\n",
      "Epoch 13/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.2131 - acc: 0.9474 - val_loss: 1.0214 - val_acc: 0.7760\n",
      "Epoch 14/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1887 - acc: 0.9581 - val_loss: 1.1470 - val_acc: 0.7320\n",
      "Epoch 15/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1791 - acc: 0.9581 - val_loss: 1.4716 - val_acc: 0.6760\n",
      "Epoch 16/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.2724 - acc: 0.9194 - val_loss: 1.0690 - val_acc: 0.7680\n",
      "Epoch 17/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.1628 - acc: 0.9600 - val_loss: 1.1464 - val_acc: 0.7480\n",
      "Epoch 18/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.1477 - acc: 0.9644 - val_loss: 1.1113 - val_acc: 0.7720\n",
      "Epoch 19/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.1326 - acc: 0.9699 - val_loss: 1.2032 - val_acc: 0.7520\n",
      "Epoch 20/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1262 - acc: 0.9714 - val_loss: 1.2229 - val_acc: 0.7480\n",
      "Epoch 21/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1184 - acc: 0.9701 - val_loss: 1.2623 - val_acc: 0.7760\n",
      "Epoch 22/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.1131 - acc: 0.9714 - val_loss: 1.3081 - val_acc: 0.7440\n",
      "Epoch 23/25\n",
      "4750/4750 [==============================] - 34s 7ms/step - loss: 0.1095 - acc: 0.9728 - val_loss: 1.2967 - val_acc: 0.7520\n",
      "Epoch 24/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1073 - acc: 0.9724 - val_loss: 1.3297 - val_acc: 0.7600\n",
      "Epoch 25/25\n",
      "4750/4750 [==============================] - 33s 7ms/step - loss: 0.1079 - acc: 0.9728 - val_loss: 1.3288 - val_acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf275d50b8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=25,\n",
    "          batch_size=64,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Mislabelled Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Prediction for: Great phone to replace my lost phone. The only thing is the volume up button does not work, but I can still go into settings to adjust. Other than that, it does the job until I am eligible to upgrade my phone again.Thaanks!\n",
      "Predicted Rating 2\n",
      "Wrong Prediction for: The charging port was loose. I got that soldered in. Then needed a new battery as well. $100 later (not including cost of purchase) I have a usable phone. The phone should not have been sold in the state it was in.\n",
      "Predicted Rating 4\n",
      "Wrong Prediction for: Phone looks good but wouldn't stay charged, had to buy new battery. Still couldn't stay charged long.so I trashed it.MONEY lost, never again will I buy from this person! !!!\n",
      "Predicted Rating 3\n"
     ]
    }
   ],
   "source": [
    "prediction_val = model.predict(x_val[:51])\n",
    "for i in range(10):\n",
    "    num = np.argmax(prediction_val[i])\n",
    "    if (num != np.argmax(y_val[i])):\n",
    "        print (\"Wrong Prediction for: \" + df['Reviews'][i])\n",
    "        print (\"Predicted Rating \" + str(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Own Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid waste crap not Predicted Rating: 0\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['Stupid waste crap not'])\n",
    "m = x_test.shape[0]  # Number of example to test\n",
    "#Initialize matrix to hold embedding values\n",
    "x_test_indices = np.zeros((m, 300)) #300 is the maximum length\n",
    "for i in range(m): \n",
    "    sentence_words = x_test[i].lower().split()\n",
    "    j = 0\n",
    "    # Loop over the words of sentence_words\n",
    "    for w in sentence_words:\n",
    "    # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "        x_test_indices[i, j] = word_index[w]\n",
    "        j = j + 1\n",
    "print(x_test[0] + ' Predicted Rating: ' + str(np.argmax(model.predict(x_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 100)          916700    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300, 128)          117248    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,166,177\n",
      "Trainable params: 249,477\n",
      "Non-trainable params: 916,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(300,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "X = LSTM(units=128, return_sequences=True)(embedded_sequences)\n",
    "X = Dropout(0.5)(X)\n",
    "X = LSTM(units=128, return_sequences=False)(X)\n",
    "X = Dropout(0.5)(X)\n",
    "X = Dense(5)(X)\n",
    "X = Activation('softmax')(X)\n",
    "model = Model(inputs=sequence_input, outputs=X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4750/4750 [==============================] - 319s 67ms/step - loss: 1.1538 - acc: 0.5926\n",
      "Epoch 2/5\n",
      "4750/4750 [==============================] - 327s 69ms/step - loss: 1.0741 - acc: 0.6248\n",
      "Epoch 3/5\n",
      "4750/4750 [==============================] - 335s 71ms/step - loss: 1.0077 - acc: 0.6482\n",
      "Epoch 4/5\n",
      "4750/4750 [==============================] - 335s 71ms/step - loss: 0.9819 - acc: 0.6505\n",
      "Epoch 5/5\n",
      "4750/4750 [==============================] - 332s 70ms/step - loss: 0.9219 - acc: 0.6653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf2edd72e8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "                 epochs=5, \n",
    "                 batch_size=64, \n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3s 12ms/step\n",
      "Test Accuracy with LSTM: 0.667999999046\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy with LSTM: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never going to buy this phone again Predicted Rating:4\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['never going to buy this phone again'])\n",
    "m = x_test.shape[0]  # Number of example to test\n",
    "#Initialize matrix to hold embedding values\n",
    "x_test_indices = np.zeros((m, 300)) #300 is the maximum length\n",
    "for i in range(m): \n",
    "    sentence_words = x_test[i].lower().split()\n",
    "    j = 0\n",
    "    # Loop over the words of sentence_words\n",
    "    for w in sentence_words:\n",
    "    # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "        x_test_indices[i, j] = word_index[w]\n",
    "        j = j + 1\n",
    "print(x_test[0] + ' Predicted Rating:' + str(np.argmax(model.predict(x_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a GRU RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 100)          916700    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300, 128)          87936     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,103,969\n",
      "Trainable params: 187,269\n",
      "Non-trainable params: 916,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(300,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "X = GRU(units=128, activation = 'relu', return_sequences=True)(embedded_sequences)\n",
    "X = Dropout(0.5)(X)\n",
    "X = GRU(units=128, activation = 'relu', return_sequences=False)(X)\n",
    "X = Dropout(0.5)(X)\n",
    "X = Dense(5)(X)\n",
    "X = Activation('softmax')(X)\n",
    "model = Model(inputs=sequence_input, outputs=X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4750/4750 [==============================] - 170s 36ms/step - loss: 1.2651 - acc: 0.5347\n",
      "Epoch 2/5\n",
      "4750/4750 [==============================] - 171s 36ms/step - loss: 1.0375 - acc: 0.6364\n",
      "Epoch 3/5\n",
      "4750/4750 [==============================] - 170s 36ms/step - loss: 0.9134 - acc: 0.6735\n",
      "Epoch 4/5\n",
      "4750/4750 [==============================] - 172s 36ms/step - loss: 0.8656 - acc: 0.6874\n",
      "Epoch 5/5\n",
      "4750/4750 [==============================] - 171s 36ms/step - loss: 0.7963 - acc: 0.7042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bf4ac15160>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "                 epochs=5, \n",
    "                 batch_size=32, \n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3s 12ms/step\n",
      "Test Accuracy with LSTM: 0.675999999046\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_val, y_val)\n",
    "print(\"Test Accuracy with LSTM: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never going to buy this phone again Predicted Rating:4\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['never going to buy this phone again'])\n",
    "m = x_test.shape[0]  # Number of example to test\n",
    "#Initialize matrix to hold embedding values\n",
    "x_test_indices = np.zeros((m, 300)) #300 is the maximum length\n",
    "for i in range(m): \n",
    "    sentence_words = x_test[i].lower().split()\n",
    "    j = 0\n",
    "    # Loop over the words of sentence_words\n",
    "    for w in sentence_words:\n",
    "    # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "        x_test_indices[i, j] = word_index[w]\n",
    "        j = j + 1\n",
    "print(x_test[0] + ' Predicted Rating:' + str(np.argmax(model.predict(x_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the Model to the Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Model from the Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(X, Y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
